{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "import requests\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import ta\n",
    "\n",
    "def get_data(url, index, proxy):    \n",
    "    global results\n",
    "    global threads\n",
    "        \n",
    "    if proxy == None:\n",
    "        res = requests.get(url, timeout=2)\n",
    "    else:\n",
    "        proxies = {\n",
    "          \"http\": \"http://\" + proxy,\n",
    "          \"https\": \"https://\" + proxy,\n",
    "        }\n",
    "        res = requests.get(url, proxies=proxies, timeout=2)\n",
    "        \n",
    "    results[index] = pd.DataFrame(json.loads(res.text))\n",
    "\n",
    "def get_df(start_time, proxy=None, total_range=30):\n",
    "    global threads\n",
    "    global results\n",
    "    \n",
    "    start_time = pd.to_datetime(start_time).tz_localize(None)\n",
    "    \n",
    "    if start_time.date() == datetime.datetime.utcnow().date():\n",
    "        urls = [\"https://www.bitmex.com/api/v1/trade?symbol=XBTUSD&count={}&start={}&reverse=false&startTime={}\".format(1000, i * 1000, start_time) for i in range(total_range)]\n",
    "    else:\n",
    "        urls = [\"https://www.bitmex.com/api/v1/trade?symbol=XBTUSD&count={}&start={}&reverse=false&startTime={}&endTime={}\".format(1000, i * 1000, start_time, pd.to_datetime(start_time.date() + pd.Timedelta(days=1))) for i in range(total_range)]\n",
    "    \n",
    "    threads = [None] * len(urls)\n",
    "    results = [None] * len(urls)\n",
    "    \n",
    "    for i in range(len(threads)):\n",
    "        threads[i] = threading.Thread(target=get_data, args=(urls[i], i, proxy))\n",
    "        threads[i].start()\n",
    "    \n",
    "    for i in range(len(threads)):\n",
    "        threads[i].join()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for curr_df in results:\n",
    "        df = df.append(curr_df, ignore_index=True)\n",
    "                    \n",
    "    return df\n",
    "\n",
    "def manual_scrape(scrape_from, sleep=True):\n",
    "    print(\"Manual scrape for {}\".format(scrape_from))\n",
    "    proxy_df = pd.read_csv('proxies', sep=':', header=None)\n",
    "    proxy_df.columns = ['proxy', 'port', 'username', 'password']\n",
    "\n",
    "    proxy_df['proxy_string'] =  proxy_df['username'] + \":\" + proxy_df['password'] + \"@\" + proxy_df['proxy'] + \":\" + proxy_df['port'].astype(str)\n",
    "    proxy_list = list(proxy_df['proxy_string'])\n",
    "    at_once = len(proxy_list) + 1\n",
    "    all_df = pd.DataFrame()\n",
    "    completed = False\n",
    "    \n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(at_once):\n",
    "            if i == 0:\n",
    "                curr_df = get_df(scrape_from)\n",
    "            else:\n",
    "                curr_df = get_df(scrape_from, proxy=proxy_list[i-1])\n",
    "                \n",
    "            all_df = all_df.append(curr_df, ignore_index=True)\n",
    "            all_df = all_df.dropna(subset=['timestamp'], how='all')\n",
    "            \n",
    "            scrape_from = all_df.iloc[-1]['timestamp']\n",
    "            print(\"Got {} data till {}\".format(len(curr_df), scrape_from))\n",
    "            \n",
    "            if len(curr_df) < 1000:\n",
    "                completed = True\n",
    "                break\n",
    "         \n",
    "        total_time_taken = time.time() - start_time\n",
    "        \n",
    "        to_sleep = int(60 - total_time_taken) + 1\n",
    "        \n",
    "        if completed == True:\n",
    "            break\n",
    "\n",
    "        if to_sleep > 0:\n",
    "            if sleep == True:\n",
    "                print(\"Sleeping {} seconds\".format(to_sleep))\n",
    "                time.sleep(to_sleep)\n",
    "        else:\n",
    "            print(\"No need to sleep\")\n",
    "            \n",
    "    \n",
    "    all_df['timestamp'] = pd.to_datetime(all_df['timestamp'])\n",
    "    all_df['timestamp'] = all_df['timestamp'].dt.tz_localize(None)\n",
    "    all_df = all_df.sort_values('timestamp').reset_index(drop=True)\n",
    "            \n",
    "    return all_df\n",
    "\n",
    "def aws_scrape(name):\n",
    "    print(\"AWS Scrape for {}\".format(name))\n",
    "    url = \"https://s3-eu-west-1.amazonaws.com/public.bitmex.com/data/trade/{}\".format(name)\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    with open('temp', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "        \n",
    "    df = pd.read_csv('temp', compression='gzip')\n",
    "    os.remove('temp')\n",
    "    aws_df = df[df['symbol'] == 'XBTUSD']\n",
    "    aws_df['timestamp'] = pd.to_datetime(aws_df['timestamp'], format=\"%Y-%m-%dD%H:%M:%S.%f\")\n",
    "    aws_df = aws_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    return aws_df\n",
    "\n",
    "def get_bitmex_data(start, end, sleep=True):\n",
    "    all_df = []\n",
    "\n",
    "    for scrape_date in pd.date_range(start, end):\n",
    "        if scrape_date.date() == datetime.datetime.utcnow().date() - pd.Timedelta(days=1):\n",
    "            curr_time = datetime.datetime.utcnow()\n",
    "            if curr_time.time() > datetime.time(5,41):\n",
    "                df = aws_scrape(scrape_date.strftime(\"%Y%m%d.csv.gz\"))\n",
    "            else:\n",
    "                df = manual_scrape(scrape_date, sleep=sleep)\n",
    "        elif scrape_date.date() == datetime.datetime.utcnow().date():\n",
    "            df = manual_scrape(scrape_date,  sleep=sleep)\n",
    "        else:\n",
    "            df = aws_scrape(scrape_date.strftime(\"%Y%m%d.csv.gz\"))\n",
    "\n",
    "\n",
    "        all_df.append(df)\n",
    "    \n",
    "    return pd.concat(all_df, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last one not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_trades():\n",
    "    end = pd.to_datetime(datetime.datetime.utcnow()).date()\n",
    "    original_start = end - pd.Timedelta(days=20)\n",
    "\n",
    "    if os.path.isfile('data/trades.csv'):\n",
    "        start = pd.to_datetime(subprocess.check_output([\"tail\", \"-1\", \"data/trades.csv\"]).decode().split(\",\")[0])\n",
    "    else:\n",
    "        start = original_start\n",
    "\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            end = pd.to_datetime(datetime.datetime.utcnow())\n",
    "            print(\"{} to {}\".format(start, end))\n",
    "            df = get_bitmex_data(start, end)\n",
    "            df = df[['timestamp', 'symbol', 'side', 'size', 'price', 'homeNotional', 'foreignNotional']]\n",
    "            \n",
    "            if not os.path.isfile('data/trades.csv'):\n",
    "                df.to_csv(\"data/trades.csv\", index=None)\n",
    "            else:\n",
    "                df.to_csv(\"data/trades.csv\", index=None, header=None, mode='a')\n",
    "                \n",
    "            break\n",
    "        except:\n",
    "            print(\"Exception. Retrying in 5 secs\")\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-18 15:06:00.860000 to 2021-01-18 15:11:27.711945\n",
      "Manual scrape for 2021-01-18 15:06:00.860000\n",
      "Got 990 data till 2021-01-18T15:11:28.167Z\n"
     ]
    }
   ],
   "source": [
    "update_trades()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_traders(df):\n",
    "    df = df[['timestamp', 'side', 'homeNotional', 'foreignNotional']]\n",
    "    df = df.groupby(['timestamp', 'side']).sum() \n",
    "    df = df.reset_index()\n",
    "    df = df[df['foreignNotional'] > 500]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['price'] = df['foreignNotional']/df['homeNotional']\n",
    "    df = df.sort_values('timestamp')\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def get_features(curr_df):\n",
    "    ser = {}\n",
    "    curr_df = curr_df.sort_values('timestamp')\n",
    "    \n",
    "    if len(curr_df) > 0:\n",
    "        ser['open'] = curr_df.iloc[0]['price']\n",
    "        ser['high'] = curr_df['price'].max()\n",
    "        ser['low'] = curr_df['price'].min()\n",
    "        ser['close'] = curr_df.iloc[-1]['price']\n",
    "        ser['volume'] = curr_df['foreignNotional'].sum()\n",
    "    else:\n",
    "        ser['open'] = np.nan\n",
    "        ser['high'] = np.nan\n",
    "        ser['low'] = np.nan\n",
    "        ser['close'] = np.nan\n",
    "        ser['volume'] = np.nan\n",
    "        \n",
    "    buy_orders = curr_df[curr_df['side'] == 'Buy']\n",
    "    sell_orders = curr_df[curr_df['side'] == 'Sell']\n",
    "\n",
    "    total_buy = buy_orders['homeNotional'].sum()\n",
    "    total_sell = sell_orders['homeNotional'].sum()\n",
    "    total = total_buy + total_sell\n",
    "\n",
    "    ser['buy_percentage'] = total_buy/total\n",
    "    ser['buy_volume'] = total_buy\n",
    "    ser['all_volume'] = total\n",
    "    \n",
    "    readable_bins = []\n",
    "    \n",
    "\n",
    "    readable_bins = [0, 2, 10, np.inf]\n",
    "        \n",
    "    readable_labels = ['small', 'medium', 'large']\n",
    "    curr_df['new_range'] = pd.cut(curr_df['homeNotional'], readable_bins, include_lowest=True, labels=readable_labels).astype(str)\n",
    "    \n",
    "        \n",
    "    for curr_range in set(readable_labels):\n",
    "        group = curr_df[curr_df['new_range'] == curr_range]\n",
    "        ser[\"percentage_{}\".format(curr_range)] = group['homeNotional'].sum()/total\n",
    "        buy_orders = group[group['side'] == 'Buy']\n",
    "        ser['buy_percentage_{}'.format(curr_range)] = (buy_orders['homeNotional'].sum())/group['homeNotional'].sum()\n",
    "\n",
    "    \n",
    "        \n",
    "    return pd.Series(ser)\n",
    "\n",
    "def get_features_from_sig(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    minute_only = df['timestamp'].dt.minute.astype(str)\n",
    "    minute_only_two = minute_only.apply(lambda x: str(x)[1:]) #there is a mistake here.\n",
    "    df = df[~((minute_only == '9') | (minute_only_two == '9') | (minute_only == '8')  | (minute_only_two == '8'))]\n",
    "\n",
    "    features = df.groupby(pd.Grouper(key='timestamp', freq=\"10Min\", label='left')).apply(get_features)\n",
    "    features = features.reset_index()\n",
    "\n",
    "    features['timestamp'] = pd.to_datetime(features['timestamp'])\n",
    "    features = features.drop_duplicates(subset=['timestamp'])\n",
    "    features = features.sort_values('timestamp')\n",
    "    features = features.dropna()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/trades.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "last_date = df['timestamp'].iloc[-1]\n",
    "\n",
    "#remove after last full\n",
    "time_df = pd.DataFrame(pd.Series({'Time': last_date})).T\n",
    "prev_date = time_df.groupby(pd.Grouper(key='Time', freq=\"10Min\", label='left')).sum().index[0]\n",
    "\n",
    "\n",
    "minute = str(last_date.time().minute)\n",
    "\n",
    "if len(minute) == 1:\n",
    "    minute_only = int(minute)\n",
    "else:\n",
    "    minute_only = int(minute[1:])\n",
    "    \n",
    "if (minute_only >= 8):\n",
    "    prev_date = prev_date + pd.Timedelta(minutes=10)\n",
    "\n",
    "have_till = prev_date\n",
    "df = df[df['timestamp'] < have_till]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove already calculated\n",
    "startTime = df.iloc[0]['timestamp']\n",
    "\n",
    "time_df = pd.DataFrame(pd.Series({'Time': startTime})).T\n",
    "startTime = time_df.groupby(pd.Grouper(key='Time', freq=\"10Min\", label='left')).sum().index[0]\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile('data/features.csv'):\n",
    "    startTime = pd.to_datetime(pd.read_csv('data/features.csv').iloc[-1]['timestamp']) + pd.Timedelta(minutes=10)\n",
    "\n",
    "df = df[df['timestamp'] >= startTime]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-990a85e2f421>:53: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ser['buy_percentage_{}'.format(curr_range)] = (buy_orders['homeNotional'].sum())/group['homeNotional'].sum()\n",
      "<ipython-input-4-990a85e2f421>:53: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ser['buy_percentage_{}'.format(curr_range)] = (buy_orders['homeNotional'].sum())/group['homeNotional'].sum()\n",
      "<ipython-input-4-990a85e2f421>:53: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ser['buy_percentage_{}'.format(curr_range)] = (buy_orders['homeNotional'].sum())/group['homeNotional'].sum()\n",
      "<ipython-input-4-990a85e2f421>:53: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ser['buy_percentage_{}'.format(curr_range)] = (buy_orders['homeNotional'].sum())/group['homeNotional'].sum()\n"
     ]
    }
   ],
   "source": [
    "#calculate and save features\n",
    "df = get_significant_traders(df)\n",
    "features = get_features_from_sig(df)\n",
    "\n",
    "features['change'] = ((features['close'] - features['open'])/features['open']) * 100\n",
    "features = features[['timestamp', 'open', 'high', 'low', 'close', 'volume', 'change', 'percentage_large', 'buy_percentage_large']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('data/features.csv'):\n",
    "    old_features = pd.read_csv('data/features.csv')\n",
    "    old_features['timestamp'] = pd.to_datetime(old_features['timestamp'])\n",
    "    features = pd.concat([old_features, features])\n",
    "    \n",
    "features['macd'] = ta.trend.macd_signal(features['close'])\n",
    "features['rsi'] = ta.momentum.rsi(features['close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.isfile('data/features.csv'):\n",
    "    features.to_csv('data/features.csv', header=None, mode='a', index=None)\n",
    "else:\n",
    "    features.to_csv('data/features.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run update_trade every 7th minute and at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after done, run update trade again and backtest verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit5e026c37115b47dab948fc9d59355aca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
