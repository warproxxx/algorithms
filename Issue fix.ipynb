{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "neutral-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "import requests\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import ta\n",
    "\n",
    "import pytz\n",
    "\n",
    "from arctic import Arctic, TICK_STORE\n",
    "from arctic.date import DateRange\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import requests\n",
    "import redis\n",
    "\n",
    "import uuid\n",
    "\n",
    "from algos.daddy.backtest import perform_backtest\n",
    "\n",
    "store = Arctic('localhost')\n",
    "\n",
    "if store.library_exists('daddy') == False:\n",
    "    store.initialize_library('daddy', lib_type=TICK_STORE)\n",
    "\n",
    "library = store['daddy']\n",
    "library._chunk_size = 500000\n",
    "\n",
    "def get_data(url, index, proxy):    \n",
    "    global results\n",
    "    global threads\n",
    "        \n",
    "    if proxy == None:\n",
    "        res = requests.get(url, timeout=2)\n",
    "    else:\n",
    "        proxies = {\n",
    "          \"http\": \"http://\" + proxy,\n",
    "          \"https\": \"https://\" + proxy,\n",
    "        }\n",
    "        res = requests.get(url, proxies=proxies, timeout=2)\n",
    "    \n",
    "    results[index] = pd.DataFrame(json.loads(res.text))\n",
    "\n",
    "def get_df(start_time, symbol, proxy=None, total_range=30):\n",
    "    global threads\n",
    "    global results\n",
    "    \n",
    "    start_time = pd.to_datetime(start_time).tz_localize(None)\n",
    "    \n",
    "    if start_time.date() == datetime.datetime.utcnow().date():\n",
    "        urls = [\"https://www.bitmex.com/api/v1/trade?symbol={}USD&count={}&start={}&reverse=false&startTime={}\".format(symbol, 1000, i * 1000, start_time) for i in range(total_range)]\n",
    "    else:\n",
    "        urls = [\"https://www.bitmex.com/api/v1/trade?symbol={}USD&count={}&start={}&reverse=false&startTime={}&endTime={}\".format(symbol, 1000, i * 1000, start_time, pd.to_datetime(start_time.date() + pd.Timedelta(days=1))) for i in range(total_range)]\n",
    "    \n",
    "    threads = [None] * len(urls)\n",
    "    results = [None] * len(urls)\n",
    "    \n",
    "    for i in range(len(threads)):\n",
    "        threads[i] = threading.Thread(target=get_data, args=(urls[i], i, proxy))\n",
    "        threads[i].start()\n",
    "    \n",
    "    for i in range(len(threads)):\n",
    "        threads[i].join()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for curr_df in results:\n",
    "        df = df.append(curr_df, ignore_index=True)\n",
    "                    \n",
    "    return df\n",
    "\n",
    "def manual_scrape(scrape_from, symbol, sleep=True):\n",
    "    print(\"Manual scrape for {}\".format(scrape_from))\n",
    "    proxy_df = pd.read_csv('proxies_{}'.format(symbol), sep=':', header=None)\n",
    "    proxy_df.columns = ['proxy', 'port', 'username', 'password']\n",
    "\n",
    "    proxy_df['proxy_string'] =  proxy_df['username'] + \":\" + proxy_df['password'] + \"@\" + proxy_df['proxy'] + \":\" + proxy_df['port'].astype(str)\n",
    "    proxy_list = list(proxy_df['proxy_string'])\n",
    "    at_once = len(proxy_list) + 1\n",
    "    all_df = pd.DataFrame()\n",
    "    completed = False\n",
    "    \n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(at_once):\n",
    "            if i == 0:\n",
    "                curr_df = get_df(scrape_from, symbol)\n",
    "            else:\n",
    "                curr_df = get_df(scrape_from, symbol, proxy=proxy_list[i-1])\n",
    "                \n",
    "            all_df = all_df.append(curr_df, ignore_index=True)\n",
    "            all_df = all_df.dropna(subset=['timestamp'], how='all')\n",
    "            \n",
    "            scrape_from = all_df.iloc[-1]['timestamp']\n",
    "            print(\"Got {} data till {}\".format(len(curr_df), scrape_from))\n",
    "            \n",
    "            if len(curr_df) < 1000:\n",
    "                completed = True\n",
    "                break\n",
    "         \n",
    "        total_time_taken = time.time() - start_time\n",
    "        \n",
    "        to_sleep = int(60 - total_time_taken) + 1\n",
    "        \n",
    "        if completed == True:\n",
    "            break\n",
    "\n",
    "        if to_sleep > 0:\n",
    "            if sleep == True:\n",
    "                print(\"Sleeping {} seconds\".format(to_sleep))\n",
    "                time.sleep(to_sleep)\n",
    "        else:\n",
    "            print(\"No need to sleep\")\n",
    "            \n",
    "    \n",
    "    all_df['timestamp'] = pd.to_datetime(all_df['timestamp'])\n",
    "    all_df['timestamp'] = all_df['timestamp'].dt.tz_localize(None)\n",
    "    all_df = all_df.sort_values('timestamp').reset_index(drop=True)\n",
    "            \n",
    "    return all_df\n",
    "\n",
    "def aws_scrape(name, symbol):\n",
    "    print(\"AWS Scrape for {}\".format(name))\n",
    "    url = \"https://s3-eu-west-1.amazonaws.com/public.bitmex.com/data/trade/{}\".format(name)\n",
    "    r = requests.get(url)\n",
    "    uid = uuid.uuid4()\n",
    "    temp = uid.hex[:8]\n",
    "    \n",
    "    with open(temp, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "        \n",
    "    df = pd.read_csv(temp, compression='gzip')\n",
    "    os.remove(temp)\n",
    "    aws_df = df[df['symbol'] == '{}USD'.format(symbol)]\n",
    "    aws_df['timestamp'] = pd.to_datetime(aws_df['timestamp'], format=\"%Y-%m-%dD%H:%M:%S.%f\")\n",
    "    aws_df = aws_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    return aws_df\n",
    "\n",
    "def get_bitmex_data(start, end, symbol, sleep=True):\n",
    "    all_df = []\n",
    "\n",
    "    for scrape_date in pd.date_range(start, end):\n",
    "        if scrape_date.date() == datetime.datetime.utcnow().date() - pd.Timedelta(days=1):\n",
    "            curr_time = datetime.datetime.utcnow()\n",
    "            if curr_time.time() > datetime.time(5,41):\n",
    "                df = aws_scrape(scrape_date.strftime(\"%Y%m%d.csv.gz\"), symbol)\n",
    "            else:\n",
    "                df = manual_scrape(scrape_date, symbol, sleep=sleep)\n",
    "        elif scrape_date.date() == datetime.datetime.utcnow().date():\n",
    "            df = manual_scrape(scrape_date, symbol, sleep=sleep)\n",
    "        else:\n",
    "            df = aws_scrape(scrape_date.strftime(\"%Y%m%d.csv.gz\"), symbol)\n",
    "\n",
    "        all_df.append(df)\n",
    "    \n",
    "    return pd.concat(all_df, axis=0)\n",
    "\n",
    "def update_trades(symbol='XBT'):\n",
    "    end = pd.to_datetime(datetime.datetime.utcnow()).date()\n",
    "    original_start = end - pd.Timedelta(days=20)\n",
    "    \n",
    "    try:\n",
    "        start = pd.to_datetime(library.max_date('{}_trades'.format(symbol)).astimezone(pytz.UTC)).tz_localize(None)\n",
    "        \n",
    "        if start.hour == 23 and start.minute >= 58:\n",
    "            start = pd.to_datetime(start.date() + pd.Timedelta(days=1))\n",
    "    except:\n",
    "        start = original_start\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            end = pd.to_datetime(datetime.datetime.utcnow())\n",
    "\n",
    "            print(\"{} to {}\".format(start, end))\n",
    "            df = get_bitmex_data(start, end, symbol=symbol)\n",
    "            df = df[['timestamp', 'symbol', 'side', 'size', 'price', 'homeNotional', 'foreignNotional']]\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.tz_localize(tz='UTC')\n",
    "            library.write('{}_trades'.format(symbol), df)               \n",
    "            break\n",
    "        except Exception as e:\n",
    "            error_mess = str(e)\n",
    "\n",
    "            if \"Document already exists with\" in error_mess:\n",
    "                splitted = error_mess.split(\" \")\n",
    "                exist_date = splitted[6].replace(\"end:\", \"\")\n",
    "                exist_date_2 = splitted[7]\n",
    "                exist_till = pd.to_datetime(exist_date + \" \" + exist_date_2)\n",
    "                new_df = df[df.index > exist_till]\n",
    "\n",
    "                if len(new_df) == 0:\n",
    "                    print(\"This timeframe already exists\")\n",
    "                else:\n",
    "                    print(\"Writing from middle\")\n",
    "                    library.write('{}_trades'.format(symbol), new_df)               \n",
    "                \n",
    "                break\n",
    "            elif 'timestamp' in str(e):\n",
    "                print(\"Timestamp error\")\n",
    "            else:\n",
    "                print(\"Exception: {}. Retrying in 20 secs\".format(str(e)))\n",
    "                time.sleep(20)\n",
    "\n",
    "def get_significant_traders(df):\n",
    "    df = df[['timestamp', 'side', 'homeNotional', 'foreignNotional']]\n",
    "    df = df.groupby(['timestamp', 'side']).sum() \n",
    "    df = df.reset_index()\n",
    "    df = df[df['foreignNotional'] > 500]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['price'] = df['foreignNotional']/df['homeNotional']\n",
    "    df = df.sort_values('timestamp')\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def get_features(curr_df):\n",
    "    ser = {}\n",
    "    curr_df = curr_df.sort_values('timestamp')\n",
    "    \n",
    "    if len(curr_df) > 0:\n",
    "        ser['open'] = curr_df.iloc[0]['price']\n",
    "        ser['high'] = curr_df['price'].max()\n",
    "        ser['low'] = curr_df['price'].min()\n",
    "        ser['close'] = curr_df.iloc[-1]['price']\n",
    "        ser['volume'] = curr_df['foreignNotional'].sum()\n",
    "    else:\n",
    "        ser['open'] = np.nan\n",
    "        ser['high'] = np.nan\n",
    "        ser['low'] = np.nan\n",
    "        ser['close'] = np.nan\n",
    "        ser['volume'] = np.nan\n",
    "        \n",
    "    buy_orders = curr_df[curr_df['side'] == 'Buy']\n",
    "    sell_orders = curr_df[curr_df['side'] == 'Sell']\n",
    "\n",
    "    total_buy = buy_orders['homeNotional'].sum()\n",
    "    total_sell = sell_orders['homeNotional'].sum()\n",
    "    total = total_buy + total_sell\n",
    "\n",
    "    ser['buy_percentage'] = total_buy/total\n",
    "    ser['buy_volume'] = total_buy\n",
    "    ser['all_volume'] = total\n",
    "    \n",
    "    readable_bins = []\n",
    "    \n",
    "\n",
    "    readable_bins = [0, 2, 10, np.inf]\n",
    "        \n",
    "    readable_labels = ['small', 'medium', 'large']\n",
    "    curr_df['new_range'] = pd.cut(curr_df['homeNotional'], readable_bins, include_lowest=True, labels=readable_labels).astype(str)\n",
    "    \n",
    "        \n",
    "    for curr_range in set(readable_labels):\n",
    "        group = curr_df[curr_df['new_range'] == curr_range]\n",
    "        ser[\"percentage_{}\".format(curr_range)] = np.nan_to_num(group['homeNotional'].sum()/total, 0)\n",
    "        buy_orders = group[group['side'] == 'Buy']\n",
    "        ser['buy_percentage_{}'.format(curr_range)] = np.nan_to_num((buy_orders['homeNotional'].sum())/group['homeNotional'].sum(), 0)\n",
    "\n",
    "    return pd.Series(ser)\n",
    "\n",
    "def get_features_from_sig(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    minute_only = df['timestamp'].dt.minute.astype(str)\n",
    "    minute_only_two = minute_only.apply(lambda x: str(x)[1:]) #there is a mistake here.\n",
    "    df = df[~((minute_only == '9') | (minute_only_two == '9') | (minute_only == '8')  | (minute_only_two == '8'))]\n",
    "\n",
    "    features = df.groupby(pd.Grouper(key='timestamp', freq=\"10Min\", label='left')).apply(get_features)\n",
    "    features = features.reset_index()\n",
    "\n",
    "    features['timestamp'] = pd.to_datetime(features['timestamp'])\n",
    "    features = features.drop_duplicates(subset=['timestamp'])\n",
    "    features = features.sort_values('timestamp')\n",
    "    return features\n",
    "\n",
    "def get_intervaled_date(startTime):\n",
    "    time_df = pd.DataFrame(pd.Series({'Time': startTime})).T\n",
    "    return time_df.groupby(pd.Grouper(key='Time', freq=\"10Min\", label='left')).sum().index[0]\n",
    "\n",
    "def update_price(symbol='XBT'):\n",
    "    start_time = \"2020-01-01\"\n",
    "\n",
    "    if os.path.isfile(\"data/{}USD_daily.csv\".format(symbol)):\n",
    "        start_time = pd.read_csv('data/{}USD_daily.csv'.format(symbol)).iloc[-1]['timestamp']\n",
    "\n",
    "    if (pd.to_datetime(start_time).date() < pd.Timestamp.utcnow().date()):\n",
    "        try:\n",
    "            new_url = 'https://www.bitmex.com/api/v1/trade/bucketed?binSize=1d&partial=false&symbol={}USD&count=500&reverse=false&startTime={}'.format(symbol, start_time)\n",
    "            res = requests.get(new_url)\n",
    "            price_df = pd.DataFrame(json.loads(res.text))\n",
    "            price_df['timestamp'] = pd.to_datetime(price_df['timestamp'])\n",
    "            price_df = price_df.set_index('timestamp').tz_localize(None).reset_index()\n",
    "\n",
    "\n",
    "            if os.path.isfile(\"data/{}USD_daily.csv\".format(symbol)):\n",
    "                old_df = pd.read_csv(\"data/{}USD_daily.csv\".format(symbol))\n",
    "                old_df['timestamp'] = pd.to_datetime(old_df['timestamp'])\n",
    "                df = pd.concat([old_df, price_df])\n",
    "                df = df.drop_duplicates(subset=['timestamp'])\n",
    "                df.to_csv('data/{}USD_daily.csv'.format(symbol), index=None)\n",
    "            else:\n",
    "                price_df.to_csv('data/{}USD_daily.csv'.format(symbol), index=None)\n",
    "        except Exception as e:\n",
    "            print(\"Exception in parameter performer: {}\".format(str(e)))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def get_trends(symbol='XBT'):\n",
    "    update_price(symbol=symbol)\n",
    "    df = pd.read_csv(\"data/{}USD_daily.csv\".format(symbol))\n",
    "    df = df[['timestamp', 'close']]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df[\"30D_volatility\"] = df['close'].rolling(30).std()/10\n",
    "    df['30D_volatility'] = df['30D_volatility'].fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    gaussian_vols = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        gaussian_vols.append(gaussian_filter(df[:idx+1]['30D_volatility'], 3.)[-1])\n",
    "\n",
    "    df['30D_volatility'] = gaussian_vols\n",
    "    \n",
    "    price_df = df.copy()\n",
    "    curr_group = \"\"\n",
    "    new_price_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(1, len(price_df)):\n",
    "        row = price_df.iloc[i]\n",
    "        curr_vol = price_df.iloc[i]['30D_volatility']\n",
    "        prev_vol = price_df.iloc[i-1]['30D_volatility']\n",
    "        three_vol = price_df.iloc[i-2]['30D_volatility']\n",
    "\n",
    "        if pd.isnull(prev_vol) == False:\n",
    "            if curr_group == \"\":\n",
    "                curr_group = price_df.iloc[i]['timestamp']\n",
    "\n",
    "\n",
    "            if (three_vol - prev_vol) * (prev_vol - curr_vol) < 0:\n",
    "                curr_group = price_df.iloc[i]['timestamp']\n",
    "\n",
    "\n",
    "\n",
    "            row['curr_group'] = curr_group\n",
    "            new_price_df = new_price_df.append(row, ignore_index=True)\n",
    "            \n",
    "    return new_price_df\n",
    "\n",
    "def save_features(features, symbol):\n",
    "    features.to_csv('data/{}_features.csv'.format(symbol), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "residential-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = pd.to_datetime(library.max_date('{}_trades'.format(symbol)).astimezone(pytz.UTC)).tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "logical-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-05-02 04:47:58.971000')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "delayed-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "minute = str(last_date.time().minute)\n",
    "\n",
    "if len(minute) == 1:\n",
    "    minute_only = int(minute)\n",
    "else:\n",
    "    minute_only = int(minute[1:])\n",
    "\n",
    "if (minute_only < 8):\n",
    "    have_till_calc = last_date - pd.Timedelta(minutes=10)\n",
    "else:\n",
    "    have_till_calc = last_date\n",
    "\n",
    "\n",
    "have_till = get_intervaled_date(have_till_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sustained-floor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-05-02 04:30:00', freq='10T')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "have_till"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "generic-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = pd.to_datetime(library.min_date('{}_trades'.format(symbol)).astimezone(pytz.UTC)).tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dated-clearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-04-11 00:00:00.843000')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "floppy-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = get_intervaled_date(min_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "victorian-mozambique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-04-11 00:00:00', freq='10T')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "blocked-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('data/{}_features.csv'.format(symbol)):\n",
    "        startTime = pd.to_datetime(subprocess.check_output([\"tail\", \"-1\", \"data/{}_features.csv\".format(symbol)]).decode().split(\",\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "criminal-terrorism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-05-02 04:30:00')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-timing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit5e026c37115b47dab948fc9d59355aca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
