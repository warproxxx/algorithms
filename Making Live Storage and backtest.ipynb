{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "import requests\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import ta\n",
    "\n",
    "import pytz\n",
    "\n",
    "from arctic import Arctic, TICK_STORE\n",
    "from arctic.date import DateRange\n",
    "\n",
    "store = Arctic('localhost')\n",
    "\n",
    "if store.library_exists('daddy') == False:\n",
    "    store.initialize_library('daddy', lib_type=TICK_STORE)\n",
    "\n",
    "library = store['daddy']\n",
    "library._chunk_size = 500000\n",
    "\n",
    "def get_data(url, index, proxy):    \n",
    "    global results\n",
    "    global threads\n",
    "        \n",
    "    if proxy == None:\n",
    "        res = requests.get(url, timeout=2)\n",
    "    else:\n",
    "        proxies = {\n",
    "          \"http\": \"http://\" + proxy,\n",
    "          \"https\": \"https://\" + proxy,\n",
    "        }\n",
    "        res = requests.get(url, proxies=proxies, timeout=2)\n",
    "        \n",
    "    results[index] = pd.DataFrame(json.loads(res.text))\n",
    "\n",
    "def get_df(start_time, proxy=None, total_range=30):\n",
    "    global threads\n",
    "    global results\n",
    "    \n",
    "    start_time = pd.to_datetime(start_time).tz_localize(None)\n",
    "    \n",
    "    if start_time.date() == datetime.datetime.utcnow().date():\n",
    "        urls = [\"https://www.bitmex.com/api/v1/trade?symbol=XBTUSD&count={}&start={}&reverse=false&startTime={}\".format(1000, i * 1000, start_time) for i in range(total_range)]\n",
    "    else:\n",
    "        urls = [\"https://www.bitmex.com/api/v1/trade?symbol=XBTUSD&count={}&start={}&reverse=false&startTime={}&endTime={}\".format(1000, i * 1000, start_time, pd.to_datetime(start_time.date() + pd.Timedelta(days=1))) for i in range(total_range)]\n",
    "    \n",
    "    threads = [None] * len(urls)\n",
    "    results = [None] * len(urls)\n",
    "    \n",
    "    for i in range(len(threads)):\n",
    "        threads[i] = threading.Thread(target=get_data, args=(urls[i], i, proxy))\n",
    "        threads[i].start()\n",
    "    \n",
    "    for i in range(len(threads)):\n",
    "        threads[i].join()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for curr_df in results:\n",
    "        df = df.append(curr_df, ignore_index=True)\n",
    "                    \n",
    "    return df\n",
    "\n",
    "def manual_scrape(scrape_from, sleep=True):\n",
    "    print(\"Manual scrape for {}\".format(scrape_from))\n",
    "    proxy_df = pd.read_csv('proxies', sep=':', header=None)\n",
    "    proxy_df.columns = ['proxy', 'port', 'username', 'password']\n",
    "\n",
    "    proxy_df['proxy_string'] =  proxy_df['username'] + \":\" + proxy_df['password'] + \"@\" + proxy_df['proxy'] + \":\" + proxy_df['port'].astype(str)\n",
    "    proxy_list = list(proxy_df['proxy_string'])\n",
    "    at_once = len(proxy_list) + 1\n",
    "    all_df = pd.DataFrame()\n",
    "    completed = False\n",
    "    \n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(at_once):\n",
    "            if i == 0:\n",
    "                curr_df = get_df(scrape_from)\n",
    "            else:\n",
    "                curr_df = get_df(scrape_from, proxy=proxy_list[i-1])\n",
    "                \n",
    "            all_df = all_df.append(curr_df, ignore_index=True)\n",
    "            all_df = all_df.dropna(subset=['timestamp'], how='all')\n",
    "            \n",
    "            scrape_from = all_df.iloc[-1]['timestamp']\n",
    "            print(\"Got {} data till {}\".format(len(curr_df), scrape_from))\n",
    "            \n",
    "            if len(curr_df) < 1000:\n",
    "                completed = True\n",
    "                break\n",
    "         \n",
    "        total_time_taken = time.time() - start_time\n",
    "        \n",
    "        to_sleep = int(60 - total_time_taken) + 1\n",
    "        \n",
    "        if completed == True:\n",
    "            break\n",
    "\n",
    "        if to_sleep > 0:\n",
    "            if sleep == True:\n",
    "                print(\"Sleeping {} seconds\".format(to_sleep))\n",
    "                time.sleep(to_sleep)\n",
    "        else:\n",
    "            print(\"No need to sleep\")\n",
    "            \n",
    "    \n",
    "    all_df['timestamp'] = pd.to_datetime(all_df['timestamp'])\n",
    "    all_df['timestamp'] = all_df['timestamp'].dt.tz_localize(None)\n",
    "    all_df = all_df.sort_values('timestamp').reset_index(drop=True)\n",
    "            \n",
    "    return all_df\n",
    "\n",
    "def aws_scrape(name):\n",
    "    print(\"AWS Scrape for {}\".format(name))\n",
    "    url = \"https://s3-eu-west-1.amazonaws.com/public.bitmex.com/data/trade/{}\".format(name)\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    with open('temp', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "        \n",
    "    df = pd.read_csv('temp', compression='gzip')\n",
    "    os.remove('temp')\n",
    "    aws_df = df[df['symbol'] == 'XBTUSD']\n",
    "    aws_df['timestamp'] = pd.to_datetime(aws_df['timestamp'], format=\"%Y-%m-%dD%H:%M:%S.%f\")\n",
    "    aws_df = aws_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    return aws_df\n",
    "\n",
    "def get_bitmex_data(start, end, sleep=True):\n",
    "    all_df = []\n",
    "\n",
    "    for scrape_date in pd.date_range(start, end):\n",
    "        if scrape_date.date() == datetime.datetime.utcnow().date() - pd.Timedelta(days=1):\n",
    "            curr_time = datetime.datetime.utcnow()\n",
    "            if curr_time.time() > datetime.time(5,41):\n",
    "                df = aws_scrape(scrape_date.strftime(\"%Y%m%d.csv.gz\"))\n",
    "            else:\n",
    "                df = manual_scrape(scrape_date, sleep=sleep)\n",
    "        elif scrape_date.date() == datetime.datetime.utcnow().date():\n",
    "            df = manual_scrape(scrape_date,  sleep=sleep)\n",
    "        else:\n",
    "            df = aws_scrape(scrape_date.strftime(\"%Y%m%d.csv.gz\"))\n",
    "\n",
    "\n",
    "        all_df.append(df)\n",
    "    \n",
    "    return pd.concat(all_df, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_trades():\n",
    "    end = pd.to_datetime(datetime.datetime.utcnow()).date()\n",
    "    original_start = end - pd.Timedelta(days=20)\n",
    "    \n",
    "    try:\n",
    "        start = pd.to_datetime(library.max_date('trades').astimezone(pytz.UTC)).tz_localize(None)\n",
    "        \n",
    "        if start.hour == 23 and start.minute >= 58:\n",
    "            start = pd.to_datetime(start.date() + pd.Timedelta(days=1))\n",
    "    except:\n",
    "        start = original_start\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            end = pd.to_datetime(datetime.datetime.utcnow())\n",
    "\n",
    "            print(\"{} to {}\".format(start, end))\n",
    "            df = get_bitmex_data(start, end)\n",
    "            df = df[['timestamp', 'symbol', 'side', 'size', 'price', 'homeNotional', 'foreignNotional']]\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.tz_localize(tz='UTC')\n",
    "            library.write('trades', df)               \n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Exception: {}. Retrying in 5 secs\".format(str(e)))\n",
    "            time.sleep(5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-19 14:06:02.445000 to 2021-01-19 14:17:19.697773\n",
      "Manual scrape for 2021-01-19 14:06:02.445000\n",
      "Got 2618 data till 2021-01-19T14:17:19.540Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NB treating all values as 'exists' - no longer sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9 data till 2021-01-19T14:17:22.222Z\n"
     ]
    }
   ],
   "source": [
    "update_trades()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_traders(df):\n",
    "    df = df[['timestamp', 'side', 'homeNotional', 'foreignNotional']]\n",
    "    df = df.groupby(['timestamp', 'side']).sum() \n",
    "    df = df.reset_index()\n",
    "    df = df[df['foreignNotional'] > 500]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['price'] = df['foreignNotional']/df['homeNotional']\n",
    "    df = df.sort_values('timestamp')\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def get_features(curr_df):\n",
    "    ser = {}\n",
    "    curr_df = curr_df.sort_values('timestamp')\n",
    "    \n",
    "    if len(curr_df) > 0:\n",
    "        ser['open'] = curr_df.iloc[0]['price']\n",
    "        ser['high'] = curr_df['price'].max()\n",
    "        ser['low'] = curr_df['price'].min()\n",
    "        ser['close'] = curr_df.iloc[-1]['price']\n",
    "        ser['volume'] = curr_df['foreignNotional'].sum()\n",
    "    else:\n",
    "        ser['open'] = np.nan\n",
    "        ser['high'] = np.nan\n",
    "        ser['low'] = np.nan\n",
    "        ser['close'] = np.nan\n",
    "        ser['volume'] = np.nan\n",
    "        \n",
    "    buy_orders = curr_df[curr_df['side'] == 'Buy']\n",
    "    sell_orders = curr_df[curr_df['side'] == 'Sell']\n",
    "\n",
    "    total_buy = buy_orders['homeNotional'].sum()\n",
    "    total_sell = sell_orders['homeNotional'].sum()\n",
    "    total = total_buy + total_sell\n",
    "\n",
    "    ser['buy_percentage'] = total_buy/total\n",
    "    ser['buy_volume'] = total_buy\n",
    "    ser['all_volume'] = total\n",
    "    \n",
    "    readable_bins = []\n",
    "    \n",
    "\n",
    "    readable_bins = [0, 2, 10, np.inf]\n",
    "        \n",
    "    readable_labels = ['small', 'medium', 'large']\n",
    "    curr_df['new_range'] = pd.cut(curr_df['homeNotional'], readable_bins, include_lowest=True, labels=readable_labels).astype(str)\n",
    "    \n",
    "        \n",
    "    for curr_range in set(readable_labels):\n",
    "        group = curr_df[curr_df['new_range'] == curr_range]\n",
    "        ser[\"percentage_{}\".format(curr_range)] = group['homeNotional'].sum()/total\n",
    "        buy_orders = group[group['side'] == 'Buy']\n",
    "        ser['buy_percentage_{}'.format(curr_range)] = (buy_orders['homeNotional'].sum())/group['homeNotional'].sum()\n",
    "\n",
    "    \n",
    "        \n",
    "    return pd.Series(ser)\n",
    "\n",
    "def get_features_from_sig(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    minute_only = df['timestamp'].dt.minute.astype(str)\n",
    "    minute_only_two = minute_only.apply(lambda x: str(x)[1:]) #there is a mistake here.\n",
    "    df = df[~((minute_only == '9') | (minute_only_two == '9') | (minute_only == '8')  | (minute_only_two == '8'))]\n",
    "\n",
    "    features = df.groupby(pd.Grouper(key='timestamp', freq=\"10Min\", label='left')).apply(get_features)\n",
    "    features = features.reset_index()\n",
    "\n",
    "    features['timestamp'] = pd.to_datetime(features['timestamp'])\n",
    "    features = features.drop_duplicates(subset=['timestamp'])\n",
    "    features = features.sort_values('timestamp')\n",
    "    features = features.dropna()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intervaled_date(startTime):\n",
    "    time_df = pd.DataFrame(pd.Series({'Time': startTime})).T\n",
    "    return time_df.groupby(pd.Grouper(key='Time', freq=\"10Min\", label='left')).sum().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = pd.to_datetime(library.max_date('trades').astimezone(pytz.UTC)).tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-01-19 14:17:22.222000')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minute = str(last_date.time().minute)\n",
    "\n",
    "if len(minute) == 1:\n",
    "    minute_only = int(minute)\n",
    "else:\n",
    "    minute_only = int(minute[1:])\n",
    "    \n",
    "if (minute_only < 8):\n",
    "    have_till_calc = last_date - pd.Timedelta(minutes=10)\n",
    "else:\n",
    "    have_till_calc = last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_till = get_intervaled_date(have_till_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-01-19 14:00:00', freq='10T')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "have_till"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = pd.to_datetime(library.min_date('trades').astimezone(pytz.UTC)).tz_localize(None)\n",
    "startTime = get_intervaled_date(min_date)\n",
    "\n",
    "if os.path.isfile('data/features.csv'):\n",
    "    startTime = pd.to_datetime(pd.read_csv('data/features.csv').iloc[-1]['timestamp']) + pd.Timedelta(minutes=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-01-19 14:00:00')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_till = have_till.tz_localize(tz='UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = startTime.tz_localize(tz='UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is some problem with appending features. discover and fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if have_till + pd.Timedelta(minutes=10) != startTime:\n",
    "    df = library.read('trades', date_range = DateRange(start=startTime, end=have_till + pd.Timedelta(minutes=10)))\n",
    "    df = df.tz_convert('UTC').tz_localize(None)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df = df.rename(columns={'index': 'timestamp'})\n",
    "    \n",
    "    #calculate and save features\n",
    "    df = get_significant_traders(df)\n",
    "    features = get_features_from_sig(df)\n",
    "\n",
    "    features['change'] = ((features['close'] - features['open'])/features['open']) * 100\n",
    "    features = features[['timestamp', 'open', 'high', 'low', 'close', 'volume', 'change', 'percentage_large', 'buy_percentage_large']]\n",
    "    \n",
    "    if os.path.isfile('data/features.csv'):\n",
    "        old_features = pd.read_csv('data/features.csv')\n",
    "        old_features['timestamp'] = pd.to_datetime(old_features['timestamp'])\n",
    "        features = pd.concat([old_features, features])\n",
    "        features = features.drop_duplicates(subset=['timestamp']).reset_index(drop=True)\n",
    "\n",
    "    features['macd'] = ta.trend.macd_signal(features['close'])\n",
    "    features['rsi'] = ta.momentum.rsi(features['close'])\n",
    "    \n",
    "    features.to_csv('data/features.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-beb0378365d2>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dupe['timestamp'] = dupe['timestamp'] + pd.Timedelta(minutes=10)\n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('data/features.csv')\n",
    "features['timestamp'] = pd.to_datetime(features['timestamp'])\n",
    "dupe = features.iloc[-1]\n",
    "dupe['timestamp'] = dupe['timestamp'] + pd.Timedelta(minutes=10)\n",
    "features = features.append(dupe, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_price():\n",
    "    start_time = \"2020-01-01\"\n",
    "\n",
    "    if os.path.isfile(\"data/btc_daily.csv\"):\n",
    "        start_time = pd.read_csv('data/btc_daily.csv').iloc[-1]['timestamp']\n",
    "\n",
    "    if (pd.to_datetime(start_time).date() < pd.Timestamp.utcnow().date()):\n",
    "        try:\n",
    "            new_url = 'https://www.bitmex.com/api/v1/trade/bucketed?binSize=1d&partial=false&symbol=XBTUSD&count=500&reverse=false&startTime={}'.format(start_time)\n",
    "            res = requests.get(new_url)\n",
    "            price_df = pd.DataFrame(json.loads(res.text))\n",
    "            price_df['timestamp'] = pd.to_datetime(price_df['timestamp'])\n",
    "            price_df = price_df.set_index('timestamp').tz_localize(None).reset_index()\n",
    "\n",
    "\n",
    "            if os.path.isfile(\"data/btc_daily.csv\"):\n",
    "                old_df = pd.read_csv(\"data/btc_daily.csv\")\n",
    "                old_df['timestamp'] = pd.to_datetime(old_df['timestamp'])\n",
    "                df = pd.concat([old_df, price_df])\n",
    "                df = df.drop_duplicates(subset=['timestamp'])\n",
    "                df.to_csv('data/btc_daily.csv', index=None)\n",
    "            else:\n",
    "                price_df.to_csv('data/btc_daily.csv', index=None)\n",
    "        except Exception as e:\n",
    "            print(\"Exception in parameter performer: {}\".format(str(e)))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at file and update if required\n",
    "def get_trends():\n",
    "    update_price()\n",
    "    df = pd.read_csv(\"data/btc_daily.csv\")\n",
    "    df = df[['timestamp', 'close']]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df[\"30D_volatility\"] = df['close'].rolling(30).std()/10\n",
    "    df['30D_volatility'] = df['30D_volatility'].fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    gaussian_vols = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        gaussian_vols.append(gaussian_filter(df[:idx+1]['30D_volatility'], 3.)[-1])\n",
    "\n",
    "    df['30D_volatility'] = gaussian_vols\n",
    "    \n",
    "    price_df = df.copy()\n",
    "    curr_group = \"\"\n",
    "    new_price_df = pd.DataFrame()\n",
    "\n",
    "    for i in range(1, len(price_df)):\n",
    "        row = price_df.iloc[i]\n",
    "        curr_vol = price_df.iloc[i]['30D_volatility']\n",
    "        prev_vol = price_df.iloc[i-1]['30D_volatility']\n",
    "        three_vol = price_df.iloc[i-2]['30D_volatility']\n",
    "\n",
    "        if pd.isnull(prev_vol) == False:\n",
    "            if curr_group == \"\":\n",
    "                curr_group = price_df.iloc[i]['timestamp']\n",
    "\n",
    "\n",
    "            if (three_vol - prev_vol) * (prev_vol - curr_vol) < 0:\n",
    "                curr_group = price_df.iloc[i]['timestamp']\n",
    "\n",
    "\n",
    "\n",
    "            row['curr_group'] = curr_group\n",
    "            new_price_df = new_price_df.append(row, ignore_index=True)\n",
    "            \n",
    "    return new_price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-804e77b52af0>:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row['curr_group'] = curr_group\n",
      "/home/warproxxx/.local/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "trends = get_trends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_group = trends.iloc[-1]['curr_group'].date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = features.iloc[-1]['timestamp'].date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if last_date.day - curr_group.day < 4:\n",
    "    curr_group = last_date - pd.Timedelta(days=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_group =pd.to_datetime(curr_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[features['timestamp'] >= curr_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algos.daddy.backtest import perform_backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = json.load(open('algos/daddy/parameters.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = perform_backtest(features, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                   Date       Value\n",
       " 0   2021-01-15 00:00:00  500.000000\n",
       " 1   2021-01-15 00:10:00  500.000000\n",
       " 2   2021-01-15 00:30:00  500.000000\n",
       " 3   2021-01-15 00:40:00  500.000000\n",
       " 4   2021-01-15 01:00:00  500.000000\n",
       " ..                  ...         ...\n",
       " 574 2021-01-19 13:30:00  496.842656\n",
       " 575 2021-01-19 13:40:00  498.976469\n",
       " 576 2021-01-19 13:50:00  498.976469\n",
       " 577 2021-01-19 14:00:00  498.619514\n",
       " 578 2021-01-19 14:10:00  498.619514\n",
       " \n",
       " [579 rows x 2 columns],\n",
       "                  Date Type         Price  Total Spent  Comission\n",
       " 0 2021-01-19 04:20:00  BUY  36662.390029   474.897104   7.123457,\n",
       " Empty DataFrame\n",
       " Columns: [Profit, Date, Value, original_value, pct_change]\n",
       " Index: [],\n",
       " 0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run[0].get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = run[0].analyzers.getbyname('tradeanalyzer').get_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoOrderedDict([('total', AutoOrderedDict([('total', 1), ('open', 1)]))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run at start. Then every minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_backtest and verify after 8th minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'open' in analysis['total']:\n",
    "    if analysis['total']['open'] == 1 and current_pos == \"NONE\":\n",
    "        print(\"Opened position from backtest_verification\")\n",
    "        lt.fill_order('buy')\n",
    "    elif analysis['total']['open'] == 0 and current_pos == \"OPEN\":\n",
    "        print(\"Closed position from backtest_verification\")\n",
    "        # lt.close_stop_order()\n",
    "        lt.fill_order('sell')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run update_trade every 7th minute and at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after done, run update trade again and backtest verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit5e026c37115b47dab948fc9d59355aca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
